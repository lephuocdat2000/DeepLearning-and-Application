{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled154.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lephuocdat2000/DeepLearning-and-Application/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kny9f6G9z3u",
        "outputId": "e32dac86-a605-41d2-f6b8-aa50489545c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfL_XLtf0dp6"
      },
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCw2Epf5A-kS",
        "outputId": "b89615b7-6c1a-4bfd-fcdc-467b2d20245b"
      },
      "source": [
        "%cd /content/drive/MyDrive/Deep Learning/Assignment3-SentimentAnalysis-with-LSTM"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Deep Learning/Assignment3-SentimentAnalysis-with-LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dewg4JU_VBk"
      },
      "source": [
        "#Load tập từ vựng và ma trận word-embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKkqOk10zGJo",
        "outputId": "3a61b0c8-57f3-4093-d179-05299ade6a40"
      },
      "source": [
        "currentDir = '/content/drive/MyDrive/Deep Learning/Assignment3-SentimentAnalysis-with-LSTM'\n",
        "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
        "print('Simplified vocabulary loaded!')\n",
        "wordsList = wordsList.tolist()\n",
        "\n",
        "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
        "wordVectors = np.float32(wordVectors)\n",
        "print ('Word embedding matrix loaded!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simplified vocabulary loaded!\n",
            "Word embedding matrix loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4lEok-wJ6Bd",
        "outputId": "03ade30d-f30a-49cb-ea17-24dca3b55427"
      },
      "source": [
        "print('Size of the vocabulary: ', len(wordsList))\n",
        "print('Size of the word embedding matrix: ', wordVectors.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the vocabulary:  19899\n",
            "Size of the word embedding matrix:  (19899, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFdV1Ab2KB98",
        "outputId": "d7326be2-461e-47ba-8bdd-58147ac04cdd"
      },
      "source": [
        "import tensorflow as tf\n",
        "maxSeqLength = 10   #Maximum length of sentence\n",
        "numDimensions = 300 #Dimensions for each word vector\n",
        "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
        "\n",
        "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
        "sentence = 'Món này ăn hoài không hề biết chán'\n",
        "words = sentence.split(\" \")\n",
        "for index,word in enumerate(words):\n",
        "    word_idx = wordsList.index(word.lower())\n",
        "    sentenceIndexes[index]=word_idx\n",
        "print(sentenceIndexes)\n",
        "#  Ma trận biểu diễn:\n",
        "print('Sentence representation of word vectors:')\n",
        "print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  119  8136  4884 18791 16614 12231 15951  3371     0     0]\n",
            "Sentence representation of word vectors:\n",
            "tf.Tensor(\n",
            "[[-0.1823 -0.0638  0.2376 ...  0.1462 -0.1092  0.0137]\n",
            " [ 0.027  -0.0542  0.1437 ... -0.0913  0.0114  0.0132]\n",
            " [ 0.021   0.0102  0.0096 ...  0.411  -0.2519  0.0151]\n",
            " ...\n",
            " [-0.0239 -0.0383  0.1734 ... -0.0677 -0.096   0.0045]\n",
            " [ 0.1882 -0.292   0.0072 ...  0.5919 -0.3094 -0.1228]\n",
            " [ 0.1882 -0.292   0.0072 ...  0.5919 -0.3094 -0.1228]], shape=(10, 300), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8l9RydNXOv-"
      },
      "source": [
        "Visual Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of9NeGNNEPkQ"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
        "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
        "numWords = []\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)       \n",
        "print('Positive files finished')\n",
        "\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)  \n",
        "print('Negative files finished')\n",
        "\n",
        "numFiles = len(numWords)\n",
        "print('The total number of files is', numFiles)\n",
        "print('The total number of words in the files is', sum(numWords))\n",
        "print('The average number of words in the files is', sum(numWords)/len(numWords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTFncENuSbzM"
      },
      "source": [
        "#Chuẩn hóa văn bản và tách từ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Tm8CVQSe4-"
      },
      "source": [
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGl2censlK6T"
      },
      "source": [
        "maxSeqLength = 180"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUPYU5SqHh0m"
      },
      "source": [
        "Chạy và lưu vào file idsMatrix.npy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4GwFonIHb_I"
      },
      "source": [
        "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
        "nFiles = 0\n",
        "# Index of Unknow word\n",
        "unk_idx = wordsList.index('UNK')\n",
        "\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            # TODO 3.2: Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
        "            if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "\n",
        "print('Positive files are indexed!')\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            # ToDo 3.2: tương tự như trên. Không khác gì hết.\n",
        "            if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "\n",
        "print('Negative files are indexed!')\n",
        "# Save ids Matrix for future uses.\n",
        "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOlq85klHmeW"
      },
      "source": [
        "Load file đã chạy để tiết kiệm thời gian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wF-oViLHhHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1299d68-e849-4615-ed91-5dd0ac45ebf0"
      },
      "source": [
        "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
        "print('Word indexes of the first review: ', ids[0])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word indexes of the first review:  [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821\n",
            " 14794  8884  6443  5767  8589 18850 15570  5596   799 11060  4222 16893\n",
            " 13078  8136  3364  4454  4756 10304  8885  3553  9782  1232 14359 10606\n",
            "   579 15522  2219 15092 14855 15253  4884  3364  5519  4558  9649   269\n",
            " 15522 12309 14855 11503  2212  4884  7155 11577  4222  5767 15076 12225\n",
            " 10774  1218  2876 19584  4558  2974 13452  5013   842 10642 17292 11895\n",
            "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
            "  7090 17292 18109 13078 16334  1238  3364  5519  4135  3553 14967  4964\n",
            " 15385  9673  2997 14855  7446  8038 11440  1345   842  5767   803 11060\n",
            " 18791  5013     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiJJrXZ0HzCU"
      },
      "source": [
        "#Xây dựng hàm lấy dữ liệu train và test theo từng batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhU1tmIFH3wb"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "def getTrainBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        if (i % 2 == 0): \n",
        "            # Pick positive samples randomly\n",
        "            num = randint(1,13999)\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            # Pick negative samples randomly\n",
        "            num = randint(15999,29999)\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels\n",
        "\n",
        "def getTestBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        num = randint(13999,15999)\n",
        "        if (num <= 14999):\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOxapFE6JXxG"
      },
      "source": [
        "#3.Xây dựng RNN Model với Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LjY8o0bJcmJ"
      },
      "source": [
        "# Initialize paramters\n",
        "numDimensions = 300\n",
        "batchSize = 64\n",
        "lstmUnits = 128\n",
        "nLayers = 2\n",
        "numClasses = 2\n",
        "iterations = 30000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4zqjkEWKp6r"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = ids.copy()\n",
        "Y = np.concatenate((np.ones((15000,1)),np.zeros((15000,1))))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.067)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ8DhdnhLgkU"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = \"training_1/weights.{epoch:04d}.hdf5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=19899,output_dim=numDimensions,weights=[wordVectors],input_length=maxSeqLength,trainable=False),\n",
        "        layers.LSTM(lstmUnits,return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(lstmUnits),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(2,activation='softmax')]\n",
        " )\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='Adam',\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M3oCiZTc98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07b0d81-0c65-46b1-a362-367cf415378e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 180, 300)          5969700   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 180, 128)          219648    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 180, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 6,321,190\n",
            "Trainable params: 351,490\n",
            "Non-trainable params: 5,969,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkYzXa-OnSKM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a22db193-3ef1-4fe8-88e6-4c43cce264a0"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batchSize, epochs=30000,callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4930: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "438/438 [==============================] - 15s 26ms/step - loss: 0.6936 - accuracy: 0.5046 - val_loss: 0.6916 - val_accuracy: 0.5057\n",
            "\n",
            "Epoch 00001: saving model to training_1/weights.0001.hdf5\n",
            "Epoch 2/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.6912 - accuracy: 0.5022 - val_loss: 0.6906 - val_accuracy: 0.5022\n",
            "\n",
            "Epoch 00002: saving model to training_1/weights.0002.hdf5\n",
            "Epoch 3/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.6873 - accuracy: 0.5305 - val_loss: 0.6913 - val_accuracy: 0.5017\n",
            "\n",
            "Epoch 00003: saving model to training_1/weights.0003.hdf5\n",
            "Epoch 4/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.6897 - accuracy: 0.5092 - val_loss: 0.6903 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00004: saving model to training_1/weights.0004.hdf5\n",
            "Epoch 5/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.6935 - accuracy: 0.5189 - val_loss: 0.6766 - val_accuracy: 0.5813\n",
            "\n",
            "Epoch 00005: saving model to training_1/weights.0005.hdf5\n",
            "Epoch 6/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.6644 - accuracy: 0.5971 - val_loss: 0.6574 - val_accuracy: 0.6126\n",
            "\n",
            "Epoch 00006: saving model to training_1/weights.0006.hdf5\n",
            "Epoch 7/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.5972 - accuracy: 0.6895 - val_loss: 0.5578 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00007: saving model to training_1/weights.0007.hdf5\n",
            "Epoch 8/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.5893 - accuracy: 0.6813 - val_loss: 0.5108 - val_accuracy: 0.7573\n",
            "\n",
            "Epoch 00008: saving model to training_1/weights.0008.hdf5\n",
            "Epoch 9/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.4748 - accuracy: 0.7774 - val_loss: 0.4553 - val_accuracy: 0.7772\n",
            "\n",
            "Epoch 00009: saving model to training_1/weights.0009.hdf5\n",
            "Epoch 10/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.4380 - accuracy: 0.7945 - val_loss: 0.4315 - val_accuracy: 0.7887\n",
            "\n",
            "Epoch 00010: saving model to training_1/weights.0010.hdf5\n",
            "Epoch 11/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.4287 - accuracy: 0.8011 - val_loss: 0.4314 - val_accuracy: 0.7951\n",
            "\n",
            "Epoch 00011: saving model to training_1/weights.0011.hdf5\n",
            "Epoch 12/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.4104 - accuracy: 0.8095 - val_loss: 0.4398 - val_accuracy: 0.7867\n",
            "\n",
            "Epoch 00012: saving model to training_1/weights.0012.hdf5\n",
            "Epoch 13/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.4004 - accuracy: 0.8155 - val_loss: 0.4120 - val_accuracy: 0.8061\n",
            "\n",
            "Epoch 00013: saving model to training_1/weights.0013.hdf5\n",
            "Epoch 14/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3853 - accuracy: 0.8224 - val_loss: 0.4178 - val_accuracy: 0.7991\n",
            "\n",
            "Epoch 00014: saving model to training_1/weights.0014.hdf5\n",
            "Epoch 15/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.3740 - accuracy: 0.8291 - val_loss: 0.4098 - val_accuracy: 0.8031\n",
            "\n",
            "Epoch 00015: saving model to training_1/weights.0015.hdf5\n",
            "Epoch 16/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3647 - accuracy: 0.8365 - val_loss: 0.4206 - val_accuracy: 0.8006\n",
            "\n",
            "Epoch 00016: saving model to training_1/weights.0016.hdf5\n",
            "Epoch 17/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3537 - accuracy: 0.8404 - val_loss: 0.4101 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00017: saving model to training_1/weights.0017.hdf5\n",
            "Epoch 18/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3458 - accuracy: 0.8461 - val_loss: 0.4095 - val_accuracy: 0.8041\n",
            "\n",
            "Epoch 00018: saving model to training_1/weights.0018.hdf5\n",
            "Epoch 19/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3317 - accuracy: 0.8535 - val_loss: 0.4326 - val_accuracy: 0.8041\n",
            "\n",
            "Epoch 00019: saving model to training_1/weights.0019.hdf5\n",
            "Epoch 20/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.3201 - accuracy: 0.8592 - val_loss: 0.4190 - val_accuracy: 0.8081\n",
            "\n",
            "Epoch 00020: saving model to training_1/weights.0020.hdf5\n",
            "Epoch 21/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.3059 - accuracy: 0.8671 - val_loss: 0.4691 - val_accuracy: 0.8031\n",
            "\n",
            "Epoch 00021: saving model to training_1/weights.0021.hdf5\n",
            "Epoch 22/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.2888 - accuracy: 0.8767 - val_loss: 0.4295 - val_accuracy: 0.8095\n",
            "\n",
            "Epoch 00022: saving model to training_1/weights.0022.hdf5\n",
            "Epoch 23/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.2720 - accuracy: 0.8859 - val_loss: 0.4653 - val_accuracy: 0.8051\n",
            "\n",
            "Epoch 00023: saving model to training_1/weights.0023.hdf5\n",
            "Epoch 24/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.2565 - accuracy: 0.8941 - val_loss: 0.4911 - val_accuracy: 0.8061\n",
            "\n",
            "Epoch 00024: saving model to training_1/weights.0024.hdf5\n",
            "Epoch 25/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.2419 - accuracy: 0.9019 - val_loss: 0.4640 - val_accuracy: 0.7887\n",
            "\n",
            "Epoch 00025: saving model to training_1/weights.0025.hdf5\n",
            "Epoch 26/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.2262 - accuracy: 0.9102 - val_loss: 0.5474 - val_accuracy: 0.8001\n",
            "\n",
            "Epoch 00026: saving model to training_1/weights.0026.hdf5\n",
            "Epoch 27/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.2053 - accuracy: 0.9220 - val_loss: 0.5346 - val_accuracy: 0.7961\n",
            "\n",
            "Epoch 00027: saving model to training_1/weights.0027.hdf5\n",
            "Epoch 28/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.1927 - accuracy: 0.9279 - val_loss: 0.5239 - val_accuracy: 0.7941\n",
            "\n",
            "Epoch 00028: saving model to training_1/weights.0028.hdf5\n",
            "Epoch 29/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.1746 - accuracy: 0.9363 - val_loss: 0.5951 - val_accuracy: 0.7951\n",
            "\n",
            "Epoch 00029: saving model to training_1/weights.0029.hdf5\n",
            "Epoch 30/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.1630 - accuracy: 0.9414 - val_loss: 0.6082 - val_accuracy: 0.7921\n",
            "\n",
            "Epoch 00030: saving model to training_1/weights.0030.hdf5\n",
            "Epoch 31/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.1500 - accuracy: 0.9479 - val_loss: 0.6491 - val_accuracy: 0.7907\n",
            "\n",
            "Epoch 00031: saving model to training_1/weights.0031.hdf5\n",
            "Epoch 32/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.1643 - accuracy: 0.9424 - val_loss: 0.5057 - val_accuracy: 0.7877\n",
            "\n",
            "Epoch 00032: saving model to training_1/weights.0032.hdf5\n",
            "Epoch 33/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.1518 - accuracy: 0.9466 - val_loss: 0.6643 - val_accuracy: 0.7966\n",
            "\n",
            "Epoch 00033: saving model to training_1/weights.0033.hdf5\n",
            "Epoch 34/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.1227 - accuracy: 0.9596 - val_loss: 0.6426 - val_accuracy: 0.7971\n",
            "\n",
            "Epoch 00034: saving model to training_1/weights.0034.hdf5\n",
            "Epoch 35/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.1091 - accuracy: 0.9653 - val_loss: 0.7785 - val_accuracy: 0.7847\n",
            "\n",
            "Epoch 00035: saving model to training_1/weights.0035.hdf5\n",
            "Epoch 36/30000\n",
            "438/438 [==============================] - 11s 24ms/step - loss: 0.1049 - accuracy: 0.9667 - val_loss: 0.8128 - val_accuracy: 0.7872\n",
            "\n",
            "Epoch 00036: saving model to training_1/weights.0036.hdf5\n",
            "Epoch 37/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.0971 - accuracy: 0.9707 - val_loss: 0.7943 - val_accuracy: 0.7902\n",
            "\n",
            "Epoch 00037: saving model to training_1/weights.0037.hdf5\n",
            "Epoch 38/30000\n",
            "438/438 [==============================] - 11s 25ms/step - loss: 0.0899 - accuracy: 0.9731 - val_loss: 0.8176 - val_accuracy: 0.7847\n",
            "\n",
            "Epoch 00038: saving model to training_1/weights.0038.hdf5\n",
            "Epoch 39/30000\n",
            "101/438 [=====>........................] - ETA: 7s - loss: 0.0851 - accuracy: 0.9743"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ac59605fb6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMWk7U4FUJ4U"
      },
      "source": [
        "Chọn weights ở epoch thứ 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEDbrURGUJAD"
      },
      "source": [
        "weight_path = 'training_1/weights.0020.hdf5'\n",
        "model.load_weights(weight_path)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMxmFvJ3e_7l"
      },
      "source": [
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())\n",
        "\n",
        "def Predict_Sentence(sentence,model):\n",
        "   cleaned = cleanSentences(sentence)\n",
        "   words = cleaned.split()\n",
        "   word_vec = np.zeros((1,len(words)))\n",
        "   for i,word in enumerate(words):\n",
        "      word_idx = wordsList.index(word)\n",
        "      word_vec[0][i] = word_idx\n",
        "   result = model.predict(word_vec)\n",
        "   if np.argmax(result[0])==1: print('Positive')\n",
        "   else: print('Negative')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSULd-G4V73M",
        "outputId": "3c3aae4d-d347-4e18-88a9-9f3e4c750f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Predict_Sentence('Món ăn gì mà dở tệ',model)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}